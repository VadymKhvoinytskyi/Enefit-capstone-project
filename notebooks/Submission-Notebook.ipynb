{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Notebook\n",
    "## Preparing and Merging Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/predict-energy-behavior-of-prosumers/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/predict-energy-behavior-of-prosumers/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Read CSVs and parse relevant date columns\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m historical_weather \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical_weather.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/predict-energy-behavior-of-prosumers/train.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n",
    "\n",
    "# Read CSVs and parse relevant date columns\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
    "historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
    "forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
    "electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
    "gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
    "weather_station_to_county_mapping = pd.read_csv(DATA_DIR + 'weather_station_to_county_mapping.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype conversion\n",
    "client.date = pd.to_datetime(client.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices.forecast_date = pd.to_datetime(electricity_prices.forecast_date)\n",
    "electricity_prices.origin_date = pd.to_datetime(electricity_prices.origin_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.origin_datetime = pd.to_datetime(forecast_weather.origin_datetime)\n",
    "forecast_weather.forecast_datetime = pd.to_datetime(forecast_weather.forecast_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gas Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices.forecast_date = pd.to_datetime(gas_prices.forecast_date)\n",
    "gas_prices.origin_date = pd.to_datetime(gas_prices.origin_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather.datetime = pd.to_datetime(historical_weather.datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data & Checking for NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.datetime = pd.to_datetime(train.datetime, format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_to_county_mapping = pd.read_csv('../data/weather_station_to_county_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/fabiendaniel/mapping-locations-and-county-codes/notebook  for county codes\n",
    "Here, they remove the 'maa' appendix from the county names. but is this really needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging (now we merge everything to train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append '_client' to merged columns\n",
    "client.columns = [f\"{column}_client\" if column not in ['data_block_id', 'county', 'is_business', 'product_type'] else column for column in client.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and client\n",
    "\n",
    "merged_df = pd.merge(train, client, on=['data_block_id', 'county', 'is_business', 'product_type'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Gas Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append _gas_prices to columns\n",
    "gas_prices.columns = [f\"{column}_gas_prices\" if column != 'data_block_id' else column for column in gas_prices.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge gas_prices\n",
    "\n",
    "merged_df = pd.merge(merged_df, gas_prices, on=['data_block_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Electricity Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time column for merging with electricity data\n",
    "merged_df['time_of_day'] = merged_df['datetime'].dt.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge electricity prices\n",
    "# the prices are available hourly -> create new column with time \n",
    "\n",
    "electricity_prices['time_of_day'] = electricity_prices.forecast_date.dt.time\n",
    "\n",
    "# append electricity_prices to column names\n",
    "electricity_prices.columns = [f\"{column}_electricity_prices\" if column not in ['time_of_day','data_block_id'] else column for column in electricity_prices.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Electricity Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge electricity_prices\n",
    "\n",
    "merged_df = pd.merge(merged_df, electricity_prices, on = ['data_block_id', 'time_of_day'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Historical Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get county and county_name from weather_station_to_county_mapping (merge on latitude and longitude)\n",
    "\n",
    "# round lat and long to avoid mismatching due to different accuracy\n",
    "historical_weather.latitude = historical_weather.latitude.round(1)\n",
    "historical_weather.longitude = historical_weather.longitude.round(1)\n",
    "\n",
    "weather_station_to_county_mapping.latitude = weather_station_to_county_mapping.latitude.round(1)\n",
    "weather_station_to_county_mapping.longitude = weather_station_to_county_mapping.longitude.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge historical weather to get counties\n",
    "merged_hist_weather = pd.merge(historical_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "# get time of day\n",
    "merged_hist_weather['time_of_day'] = merged_hist_weather['datetime'].dt.time\n",
    "\n",
    "# aggregate by county and time (summarize weather stations for same county)\n",
    "merged_hist_weather = merged_hist_weather.groupby(['county', 'time_of_day', 'datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# append _hist_weather to column names\n",
    "merged_hist_weather.columns = [f\"{column}_hist_weather\" if column not in ['county', 'time_of_day','data_block_id'] else column for column in merged_hist_weather.columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to merged_df\n",
    "merged_df = pd.merge(merged_df, merged_hist_weather, on=['data_block_id', 'time_of_day', 'county'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Forecast Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast weather\n",
    "\n",
    "#round lat and long\n",
    "forecast_weather.latitude = forecast_weather.latitude.round(1)\n",
    "forecast_weather.longitude = forecast_weather.longitude.round(1)\n",
    "\n",
    "# merge to get counties\n",
    "merged_forecast_weather = pd.merge(forecast_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "# merged_forecast_weather['time_of_day'] = merged_forecast_weather.\n",
    "\n",
    "# # aggregate for duplicate locations\n",
    "merged_forecast_weather = merged_forecast_weather.groupby(['county', 'forecast_datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# append forecast_weather to column names\n",
    "merged_forecast_weather.columns = [f\"{column}_forecast_weather\" if column not in ['county', 'forecast_datetime','data_block_id'] else column for column in merged_forecast_weather.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add EET timezone to datetime, and handle daylight-savings\n",
    "merged_df['datetime_localized'] = merged_df.datetime.dt.tz_localize('EET', ambiguous=True, nonexistent='shift_forward')\n",
    "\n",
    "# convert UTC timezone to EET timezone in forecast weather\n",
    "merged_forecast_weather['datetime_EET']  = merged_forecast_weather.forecast_datetime.dt.tz_convert('EET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge forecast_weather\n",
    "merged_df = pd.merge(merged_df, merged_forecast_weather, left_on=['data_block_id', 'datetime_localized', 'county'], right_on=['data_block_id', 'datetime_EET', 'county'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datetime(data, col=\"datetime\"):\n",
    "    # What columns are of type datetime?\n",
    "    datetime_columns = data.select_dtypes(include='datetime64').columns\n",
    "    \n",
    "    for c in datetime_columns:\n",
    "        print(f\"Timezone for {c} is {data[c].dt.tz}\")\n",
    "\n",
    "    # Adding columns for date & time\n",
    "    data['year']    = data[col].dt.year\n",
    "    # data['quarter'] = data[col].dt.quarter\n",
    "    data['month']   = data[col].dt.month\n",
    "    data['week']    = data[col].dt.isocalendar().week\n",
    "    data['hour']    = data[col].dt.hour \n",
    "\n",
    "    data['day_of_year']  = data[col].dt.day_of_year\n",
    "    data['day_of_month'] = data[col].dt.day\n",
    "    data['day_of_week']  = data[col].dt.day_of_week\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping days of the week names and converting to categorical variable\n",
    "if 'day_of_week' in merged_df.columns:\n",
    "    weekday_map = {\n",
    "        0: 'Monday',\n",
    "        1: 'Tuesday',\n",
    "        2: 'Wednesday',\n",
    "        3: 'Thursday',\n",
    "        4: 'Friday',\n",
    "        5: 'Saturday',\n",
    "        6: 'Sunday'\n",
    "    }\n",
    "    merged_df['day_of_week'] = merged_df['day_of_week'].map(weekday_map).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categories to category datetype\n",
    "\n",
    "merged_df['county'] = merged_df['county'].astype('category')\n",
    "merged_df['product_type'] = merged_df['product_type'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df for modelling\n",
    "model_df = merged_df\n",
    "\n",
    "# model is not able to handle object type\n",
    "model_df.drop('time_of_day', axis=1, inplace=True)\n",
    "\n",
    "# split datetime into meaningful features of int types\n",
    "model_df = split_datetime(model_df)\n",
    "\n",
    "# model is not able to handle datetime\n",
    "model_df = model_df.drop(model_df.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, EET]']).columns, axis=1)\n",
    "\n",
    "# drop na from target\n",
    "model_df.dropna(subset=['target'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train,  y_test = train_test_split(model_df.drop('target', axis=1), model_df['target'], test_size=0.3, random_state=0)\n",
    "drop_columns = [\n",
    "    'target',\n",
    "    'hours_ahead_forecast_weather',\n",
    "    'row_id',\n",
    "    'data_block_id',\n",
    "    'prediction_unit_id',\n",
    "    'longitude_hist_weather',\n",
    "    'longitude_forecast_weather',\n",
    "    'latitude_hist_weather',\n",
    "    'latitude_forecast_weather'\n",
    "]\n",
    "\n",
    "\n",
    "model = XGBRegressor(enable_categorical=True, max_depth=9, learning_rate=0.3)\n",
    "model.fit(model_df.drop(drop_columns, axis=1), model_df.target)\n",
    "\n",
    "# y_pred = bst.predict(X_test)\n",
    "\n",
    "## main optimisation metric\n",
    "# print('Mean absolute error test', mean_absolute_error(y_test, y_pred))\n",
    "# print('Mean absolute error train', mean_absolute_error(y_train, bst.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data / API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_prep(test, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction, weather_station_to_county_mapping):        \n",
    "\n",
    "    # Datatype conversion\n",
    "    client.date = pd.to_datetime(client.date)\n",
    "\n",
    "    ## Electricity Prices Data\n",
    "    electricity_prices.forecast_date = pd.to_datetime(electricity_prices.forecast_date)\n",
    "    electricity_prices.origin_date = pd.to_datetime(electricity_prices.origin_date)\n",
    "\n",
    "    ## Forecast Weather Data\n",
    "    forecast_weather.origin_datetime = pd.to_datetime(forecast_weather.origin_datetime)\n",
    "    forecast_weather.forecast_datetime = pd.to_datetime(forecast_weather.forecast_datetime)\n",
    "\n",
    "    ## Gas Prices Data\n",
    "    gas_prices.forecast_date = pd.to_datetime(gas_prices.forecast_date)\n",
    "    gas_prices.origin_date = pd.to_datetime(gas_prices.origin_date)\n",
    "\n",
    "    ## Historical Weather Data\n",
    "    historical_weather.datetime = pd.to_datetime(historical_weather.datetime)\n",
    "\n",
    "    ## Train Data & Checking for NULL values\n",
    "    test['datetime'] = pd.to_datetime(test.prediction_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    ## Data Merging (now we merge everything to test)\n",
    "    ### Merge Client\n",
    "    # append '_client' to merged columns\n",
    "    client.columns = [f\"{column}_client\" if column not in ['data_block_id', 'county', 'is_business', 'product_type'] else column for column in client.columns]\n",
    "\n",
    "    # merge train and client\n",
    "    merged_df = pd.merge(test, client, on=['data_block_id', 'county', 'is_business', 'product_type'], how='left')\n",
    "\n",
    "    ### Merge Gas Prices\n",
    "    # append _gas_prices to columns\n",
    "    gas_prices.columns = [f\"{column}_gas_prices\" if column != 'data_block_id' else column for column in gas_prices.columns]\n",
    "\n",
    "    # merge gas_prices\n",
    "    merged_df = pd.merge(merged_df, gas_prices, on=['data_block_id'], how='left')\n",
    "\n",
    "    ### Merge Electricity Prices\n",
    "    # add time column for merging with electricity data\n",
    "    merged_df['time_of_day'] = merged_df['datetime'].dt.time\n",
    "\n",
    "    # Merge electricity prices\n",
    "    # the prices are available hourly -> create new column with time \n",
    "    electricity_prices['time_of_day'] = electricity_prices.forecast_date.dt.time\n",
    "\n",
    "    # append electricity_prices to column names\n",
    "    electricity_prices.columns = [f\"{column}_electricity_prices\" if column not in ['time_of_day','data_block_id'] else column for column in electricity_prices.columns]\n",
    "\n",
    "    ### Merge Electricity Prices\n",
    "    # merge electricity_prices\n",
    "    merged_df = pd.merge(merged_df, electricity_prices, on = ['data_block_id', 'time_of_day'], how='left')\n",
    "\n",
    "    ### Merge Historical Weather\n",
    "    # get county and county_name from weather_station_to_county_mapping (merge on latitude and longitude)\n",
    "\n",
    "    # round lat and long to avoid mismatching due to different accuracy\n",
    "    historical_weather.latitude = historical_weather.latitude.round(1)\n",
    "    historical_weather.longitude = historical_weather.longitude.round(1)\n",
    "\n",
    "    weather_station_to_county_mapping.latitude = weather_station_to_county_mapping.latitude.round(1)\n",
    "    weather_station_to_county_mapping.longitude = weather_station_to_county_mapping.longitude.round(1)\n",
    "\n",
    "    # merge historical weather to get counties\n",
    "    merged_hist_weather = pd.merge(historical_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "    # get time of day\n",
    "    merged_hist_weather['time_of_day'] = merged_hist_weather['datetime'].dt.time\n",
    "\n",
    "    # aggregate by county and time (summarize weather stations for same county)\n",
    "    merged_hist_weather = merged_hist_weather.groupby(['county', 'time_of_day', 'datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # append _hist_weather to column names\n",
    "    merged_hist_weather.columns = [f\"{column}_hist_weather\" if column not in ['county', 'time_of_day','data_block_id'] else column for column in merged_hist_weather.columns]\n",
    "\n",
    "\n",
    "    # merge to merged_df\n",
    "    merged_df = pd.merge(merged_df, merged_hist_weather, on=['data_block_id', 'time_of_day', 'county'], how='left')\n",
    "\n",
    "    ### Merge Forecast Weather\n",
    "    # forecast weather\n",
    "\n",
    "    #round lat and long\n",
    "    forecast_weather.latitude = forecast_weather.latitude.round(1)\n",
    "    forecast_weather.longitude = forecast_weather.longitude.round(1)\n",
    "\n",
    "    # merge to get counties\n",
    "    merged_forecast_weather = pd.merge(forecast_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "    # merged_forecast_weather['time_of_day'] = merged_forecast_weather.\n",
    "\n",
    "    # # aggregate for duplicate locations\n",
    "    merged_forecast_weather = merged_forecast_weather.groupby(['county', 'forecast_datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # append forecast_weather to column names\n",
    "    merged_forecast_weather.columns = [f\"{column}_forecast_weather\" if column not in ['county', 'forecast_datetime','data_block_id'] else column for column in merged_forecast_weather.columns]\n",
    "\n",
    "\n",
    "    # add EET timezone to datetime, and handle daylight-savings\n",
    "    merged_df['datetime_localized'] = merged_df.datetime.dt.tz_localize('EET', ambiguous=True, nonexistent='shift_forward')\n",
    "\n",
    "    # convert UTC timezone to EET timezone in forecast weather\n",
    "    merged_forecast_weather['datetime_EET']  = merged_forecast_weather.forecast_datetime.dt.tz_convert('EET')\n",
    "\n",
    "    # merge forecast_weather\n",
    "    merged_df = pd.merge(merged_df, merged_forecast_weather, left_on=['data_block_id', 'datetime_localized', 'county'], right_on=['data_block_id', 'datetime_EET', 'county'], how='left')\n",
    "\n",
    "    # mapping days of the week names and converting to categorical variable\n",
    "    if 'day_of_week' in merged_df.columns:\n",
    "        weekday_map = {\n",
    "            0: 'Monday',\n",
    "            1: 'Tuesday',\n",
    "            2: 'Wednesday',\n",
    "            3: 'Thursday',\n",
    "            4: 'Friday',\n",
    "            5: 'Saturday',\n",
    "            6: 'Sunday'\n",
    "        }\n",
    "        merged_df['day_of_week'] = merged_df['day_of_week'].map(weekday_map).astype('category')\n",
    "    # encode categories to category datetype\n",
    "\n",
    "    merged_df['county'] = merged_df['county'].astype('category')\n",
    "    merged_df['product_type'] = merged_df['product_type'].astype('category')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timezone for datetime is None\n",
      "Timezone for date_client is None\n",
      "Timezone for forecast_date_gas_prices is None\n",
      "Timezone for origin_date_gas_prices is None\n",
      "Timezone for forecast_date_electricity_prices is None\n",
      "Timezone for origin_date_electricity_prices is None\n",
      "Timezone for datetime_hist_weather is None\n"
     ]
    }
   ],
   "source": [
    "# copy df for modelling\n",
    "model_df = merged_df\n",
    "\n",
    "# model is not able to handle object type\n",
    "model_df.drop('time_of_day', axis=1, inplace=True)\n",
    "\n",
    "# split datetime into meaningful features of int types\n",
    "model_df = split_datetime(model_df)\n",
    "\n",
    "# model is not able to handle datetime\n",
    "model_df = model_df.drop(model_df.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, EET]']).columns, axis=1)\n",
    "\n",
    "## drop na from target\n",
    "#model_df.dropna(subset=['target'], inplace=True)  # we dont have target in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('../data')\n",
    "\n",
    "import public_timeseries_testing_util as enefit\n",
    "\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()\n",
    "counter = 0\n",
    "for (test, revealed_targets, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "    if counter == 0:\n",
    "        print(test.head(3))\n",
    "        print(revealed_targets.head(3))\n",
    "        print(client.head(3))\n",
    "        print(historical_weather.head(3))\n",
    "        print(forecast_weather.head(3))\n",
    "        print(electricity_prices.head(3))\n",
    "        print(gas_prices.head(3))\n",
    "        print(sample_prediction.head(3))\n",
    "#    sample_prediction['target'] = 0\n",
    "    \n",
    "    prepped_df = data_prep(test, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction, weather_station_to_county_mapping)\n",
    "    \n",
    "    sample_prediction = model.predict(prepped_df)\n",
    "\n",
    "    env.predict(sample_prediction)\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
