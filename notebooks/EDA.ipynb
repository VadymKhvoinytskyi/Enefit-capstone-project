{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Living' Conclusion Gathering Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions Data Merge\n",
    "\n",
    "-  There seems to be no pattern in the null values in target column. This indicates we can drop these rows\n",
    "\n",
    "\n",
    "_County_\n",
    "- we think county 12 is weird (unknown location) and introduces NA values (not included in weather data)\n",
    "- maybe drop 12, but this may lead to other problems \n",
    "- county named 'unknown'\n",
    "- the forums say counties 16 and 0 may be weird\n",
    "\n",
    "_data_block_id_\n",
    "- we could reduce NaN and NaT values by excluding data_block_id 1 and 0 (beginning in data set)\n",
    "\n",
    "_Modelling / Time Series_\n",
    "- We are unsure about modelling (is time series model needed? maybe ARMA?) Forums suggest e.g. XGBoost\n",
    "\n",
    "\n",
    "## Conclusions EDA\n",
    "- consumption has noticeable affects by winter holidays\n",
    "- seems consumption is growing over time\n",
    "- county 0 is dominating, Tallinn located there\n",
    "- seems like temperature to production ratio changed last year\n",
    "- product_type 2 attract producers with small installed capacity thus low production\n",
    "- while product_type 3 attract the opposite cluster, producers with a lot of installed_capacity\n",
    "- Surface solar radiation seems to have a stronger correlation with target than direct_solar\n",
    "- There seems to be a 'split' around 6000 (unit?) daily mean target\n",
    "- We expected more businesses in the top-producers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv('../data/client.csv')\n",
    "client.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype conversion\n",
    "client.date = pd.to_datetime(client.date)\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoricals?\n",
    "display(client.product_type.unique())\n",
    "display(client.is_business.unique())\n",
    "display(client.county.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = client.date, y = client.data_block_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(min(client.date))\n",
    "display(max(client.date))\n",
    "display(client.data_block_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First day is '2021-09-01 00:00:00', last day is '2023-05-29 00:00:00'. There are 636 unique days, and data_block_id corresponds to date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices = pd.read_csv('../data/electricity_prices.csv')\n",
    "electricity_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices.forecast_date = pd.to_datetime(electricity_prices.forecast_date)\n",
    "electricity_prices.origin_date = pd.to_datetime(electricity_prices.origin_date)\n",
    "\n",
    "electricity_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices.forecast_date.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Electricity price forecast are available for each hour of the day (637 days * 24 hours = 15286 unique datetimes) (for 637 days, one day more than client data; somewhere there are 2h missing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather = pd.read_csv('../data/forecast_weather.csv')\n",
    "\n",
    "forecast_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.origin_datetime = pd.to_datetime(forecast_weather.origin_datetime)\n",
    "forecast_weather.forecast_datetime = pd.to_datetime(forecast_weather.forecast_datetime)\n",
    "\n",
    "forecast_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.groupby(['latitude', 'longitude']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There are 112 unique combinations of lat and long (unique weather stations). \n",
    "    So for each forecast_date, there are 112 observations (one from each station). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(forecast_weather.hours_ahead.nunique())\n",
    "display(forecast_weather.origin_datetime.nunique())\n",
    "display(forecast_weather.groupby('origin_datetime').forecast_datetime.size()/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.groupby(['origin_datetime', 'forecast_datetime']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gas Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices = pd.read_csv('../data/gas_prices.csv')\n",
    "\n",
    "gas_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices.forecast_date = pd.to_datetime(gas_prices.forecast_date)\n",
    "gas_prices.origin_date = pd.to_datetime(gas_prices.origin_date)\n",
    "\n",
    "gas_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather = pd.read_csv('../data/historical_weather.csv')\n",
    "\n",
    "historical_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather.datetime = pd.to_datetime(historical_weather.datetime)\n",
    "\n",
    "historical_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data & Checking for NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.datetime = pd.to_datetime(train.datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.datetime[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('is_consumption').agg({'target': lambda x: x.isnull().sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.target.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There seems to be no pattern in the null values in target column. This indicates we can drop these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_to_county_mapping = pd.read_csv('../data/weather_station_to_county_mapping.csv')\n",
    "\n",
    "weather_station_to_county_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_to_county_mapping.county.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_to_county_mapping.county_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/fabiendaniel/mapping-locations-and-county-codes/notebook  for county codes\n",
    "Here, they remove the 'maa' appendix from the county names. but is this really needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging (now we merge everything to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.size, train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append '_client' to merged columns\n",
    "client.columns = [f\"{column}_client\" if column not in ['data_block_id', 'county', 'is_business', 'product_type'] else column for column in client.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and client\n",
    "\n",
    "merged_df = pd.merge(train, client, on=['data_block_id', 'county', 'is_business', 'product_type'], how='left')\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a few/a lot null values, especially at the beginning and end of period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many eic counts per data_block_id?\n",
    "merged_df[merged_df.eic_count_client.isnull()].data_block_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do dates in train and client overlap? \n",
    "print(set(client.date_client.dt.date) ^ set(train.datetime.dt.date))\n",
    "print(set(train.data_block_id) ^ set(client.data_block_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_block(dbid):\n",
    "    display(\"TRAIN\", train[train['data_block_id'] == dbid])\n",
    "#     display(\"FORC WEATHER\", forecast_weather[forecast_weather['data_block_id'] == dbid])\n",
    "    display(\"CLIENT\", client[client['data_block_id'] == dbid])\n",
    "#     display(\"HIST WEATHER\", historical_weather[historical_weather['data_block_id'] == dbid])\n",
    "#     display(\"E PRICES\", electricity_prices[electricity_prices['data_block_id'] == dbid])\n",
    "#     display(\"G PRICES\", gas_prices[gas_prices['data_block_id'] == dbid])\n",
    "\n",
    "print_block(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with NULL values after merging. one source are the start and end dates, but we don't know whats happening in between and whether this is problematic.\n",
    "Maybe some client data is sporadically missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Gas Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append _gas_prices to columns\n",
    "gas_prices.columns = [f\"{column}_gas_prices\" if column != 'data_block_id' else column for column in gas_prices.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge gas_prices\n",
    "\n",
    "merged_df = pd.merge(merged_df, gas_prices, on=['data_block_id'], how='left')\n",
    "\n",
    "merged_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Electricity Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time column for merging with electricity data\n",
    "merged_df['time_of_day'] = merged_df['datetime'].dt.time\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge electricity prices\n",
    "# the prices are available hourly -> create new column with time \n",
    "\n",
    "electricity_prices['time_of_day'] = electricity_prices.forecast_date.dt.time\n",
    "\n",
    "# append electricity_prices to column names\n",
    "electricity_prices.columns = [f\"{column}_electricity_prices\" if column not in ['time_of_day','data_block_id'] else column for column in electricity_prices.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Electricity Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge electricity_prices\n",
    "\n",
    "merged_df = pd.merge(merged_df, electricity_prices, on = ['data_block_id', 'time_of_day'], how='left')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Historical Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historic weather\n",
    "\n",
    "historical_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get county and county_name from weather_station_to_county_mapping (merge on latitude and longitude)\n",
    "\n",
    "# round lat and long to avoid mismatching due to different accuracy\n",
    "historical_weather.latitude = historical_weather.latitude.round(1)\n",
    "historical_weather.longitude = historical_weather.longitude.round(1)\n",
    "\n",
    "weather_station_to_county_mapping.latitude = weather_station_to_county_mapping.latitude.round(1)\n",
    "weather_station_to_county_mapping.longitude = weather_station_to_county_mapping.longitude.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge historical weather to get counties\n",
    "merged_hist_weather = pd.merge(historical_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "# get time of day\n",
    "merged_hist_weather['time_of_day'] = merged_hist_weather['datetime'].dt.time\n",
    "\n",
    "# aggregate by county and time (summarize weather stations for same county)\n",
    "merged_hist_weather = merged_hist_weather.groupby(['county', 'time_of_day', 'datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# append _hist_weather to column names\n",
    "merged_hist_weather.columns = [f\"{column}_hist_weather\" if column not in ['county', 'time_of_day','data_block_id'] else column for column in merged_hist_weather.columns]\n",
    "\n",
    "\n",
    "merged_hist_weather.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to merged_df\n",
    "merged_df = pd.merge(merged_df, merged_hist_weather, on=['data_block_id', 'time_of_day', 'county'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Forecast Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast weather\n",
    "\n",
    "#round lat and long\n",
    "forecast_weather.latitude = forecast_weather.latitude.round(1)\n",
    "forecast_weather.longitude = forecast_weather.longitude.round(1)\n",
    "\n",
    "# merge to get counties\n",
    "merged_forecast_weather = pd.merge(forecast_weather, weather_station_to_county_mapping, on=['latitude', 'longitude'], how='left')\n",
    "# merged_forecast_weather['time_of_day'] = merged_forecast_weather.\n",
    "\n",
    "# # aggregate for duplicate locations\n",
    "merged_forecast_weather = merged_forecast_weather.groupby(['county', 'forecast_datetime', 'data_block_id']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# append forecast_weather to column names\n",
    "merged_forecast_weather.columns = [f\"{column}_forecast_weather\" if column not in ['county', 'forecast_datetime','data_block_id'] else column for column in merged_forecast_weather.columns]\n",
    "\n",
    "\n",
    "merged_forecast_weather.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.datetime.dt.tz_localize('EET', ambiguous=True, nonexistent='shift_forward')[80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.datetime.dt.tz_localize('EET', ambiguous=True, nonexistent='shift_forward')[1500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_forecast_weather.forecast_datetime.dt.tz_convert('EET')[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add EET timezone to datetime, and handle daylight-savings\n",
    "merged_df['datetime_localized'] = merged_df.datetime.dt.tz_localize('EET', ambiguous=True, nonexistent='shift_forward')\n",
    "\n",
    "# convert UTC timezone to EET timezone in forecast weather\n",
    "merged_forecast_weather['datetime_EET']  = merged_forecast_weather.forecast_datetime.dt.tz_convert('EET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_forecast_weather.query('data_block_id == 300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge forecast_weather\n",
    "merged_df = pd.merge(merged_df, merged_forecast_weather, left_on=['data_block_id', 'datetime_localized', 'county'], right_on=['data_block_id', 'datetime_EET', 'county'], how='left')\n",
    "\n",
    "merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for NULL values on merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df.query('data_block_id != 0 and data_block_id != 1').isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.query('data_block_id != 0 and data_block_id != 1')[merged_df.query('data_block_id != 0 and data_block_id != 1').isnull()].data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(data=merged_df, x='datetime', y= 'temperature_hist_weather', hue='county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(merged_df.query('data_block_id != 0 and data_block_id != 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.temperature_hist_weather.isnull()) & (merged_df.data_block_id == 200)]\n",
    "\n",
    "# .datetime.dt.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('county').aggregate(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.query('county == 11').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_to_county_mapping.query('county == 12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_df = merged_df.sample(10000)\n",
    "#sns.scatterplot(data=merged_df, x='datetime', y = 'target', hue='is_consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=subsampled_df, x='datetime', y = 'target', hue='is_consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=subsampled_df, x = 'target', hue='is_consumption')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset into production df\n",
    "\n",
    "production_df = merged_df.query('is_consumption == 0').groupby('datetime').mean(numeric_only=True)\n",
    "\n",
    "# we should aggregate target by sum, weather variables by  mean\n",
    "\n",
    "production_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=production_df, x = 'cloudcover_total_forecast_weather', y = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=production_df, x = 'direct_solar_radiation_forecast_weather', y = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=production_df, x = 'surface_solar_radiation_downwards_forecast_weather', y = 'target', hue='is_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df = merged_df.query('is_consumption == 0').sample(100000)\n",
    "sns.scatterplot(data=production_df, x = 'surface_solar_radiation_downwards_forecast_weather', y = 'target', hue='product_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Surface solar radiation seems to have a stronger correlation with target than direct_solar\n",
    "    - There seems to be a 'split' around 6000 (unit?) daily mean target\n",
    "    - We expected more businesses in the top-producers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focussing only on larger providers (is there a trend visible?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df = merged_df.query('(is_consumption == 0) and (1200 < installed_capacity_client < 1300)')\n",
    "sns.scatterplot(data=production_df, x = 'surface_solar_radiation_downwards_forecast_weather', y = 'target', hue='product_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.query('is_consumption == 0').groupby('product_type')['installed_capacity_client'].aggregate(['min', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - product_type 2 attract producers with small installed capacity thus low production\n",
    "    - while product_type 3 attract the opposite cluster, producers with a lot of installed_capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further quick Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.plot(x='datetime', y='euros_per_mwh_electricity_prices')\n",
    "plt.title('euros_per_mwh_electricity_prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(merged_df['datetime'], merged_df['lowest_price_per_mwh_gas_prices'], label = 'lowest price')\n",
    "plt.plot(merged_df['datetime'], merged_df['highest_price_per_mwh_gas_prices'], label = 'highest price')\n",
    "plt.title('highest and lowest price_per_mwh_gas_prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.plot(x='datetime', y='temperature_hist_weather')\n",
    "plt.title('temperature_hist_weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['dewpoint_hist_weather'].plot()\n",
    "plt.title('dewpoint_hist_weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['rain_hist_weather'].plot()\n",
    "plt.title('rain_hist_weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['snowfall_hist_weather'].plot()\n",
    "plt.title('snowfall_hist_weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['cloudcover_total_hist_weather'].plot()\n",
    "plt.title('cloudcover_total_hist_weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['shortwave_radiation_hist_weather','direct_solar_radiation_hist_weather', 'diffuse_radiation_hist_weather']].plot()\n",
    "plt.title('solar radiation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = merged_df.select_dtypes(include=[np.number])\n",
    "\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Plot the first graph\n",
    "merged_df.groupby('county')['target'].mean().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Average Consumption per County')\n",
    "axes[0].set_xlabel('County')\n",
    "axes[0].set_ylabel('Average Consumption')\n",
    "\n",
    "# Plot the second graph\n",
    "merged_df.groupby('product_type')['target'].mean().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Average Consumption per Product type')\n",
    "axes[1].set_xlabel('Product type')\n",
    "axes[1].set_ylabel('Average Consumption')\n",
    "\n",
    "# Plot the third graph\n",
    "merged_df.groupby('is_business')['target'].mean().plot(kind='bar', ax=axes[2])\n",
    "axes[2].set_title('Average Consumption per Business')\n",
    "axes[2].set_xlabel('Business or not')\n",
    "axes[2].set_ylabel('Average Consumption')\n",
    "\n",
    "# Adjust layout to prevent clipping of titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datetime(data, col=\"datetime\"):\n",
    "    # What columns are of type datetime?\n",
    "    datetime_columns = data.select_dtypes(include='datetime64').columns\n",
    "    \n",
    "    for c in datetime_columns:\n",
    "        print(f\"Timezone for {c} is {data[c].dt.tz}\")\n",
    "\n",
    "    # Adding columns for date & time\n",
    "    data['year']    = data[col].dt.year\n",
    "    data['quarter'] = data[col].dt.quarter\n",
    "    data['month']   = data[col].dt.month\n",
    "    data['week']    = data[col].dt.isocalendar().week\n",
    "    data['hour']    = data[col].dt.hour \n",
    "\n",
    "    data['day_of_year']  = data[col].dt.day_of_year\n",
    "    data['day_of_month'] = data[col].dt.day\n",
    "    data['day_of_week']  = data[col].dt.day_of_week\n",
    "\n",
    "    return data\n",
    "\n",
    "# calculate sum of production of business producers and the average over the day of the week\n",
    "business_production_sum_timeseries = merged_df.query('is_consumption == 0 & is_business == 1')[[\"datetime\", \"target\"]].groupby(\"datetime\").sum()\n",
    "business_production_sum_timeseries.reset_index(inplace=True)\n",
    "business_production_sum_timeseries = split_datetime(business_production_sum_timeseries, \"datetime\")\n",
    "# Have a look at the production average per day of the week\n",
    "business_production_av_day_of_week = business_production_sum_timeseries.groupby([\"day_of_week\", \"hour\"])[[\"target\"]].mean()\n",
    "business_production_av_day_of_week.reset_index(inplace=True)\n",
    "\n",
    "# calculate sum of production of non-business producers and the average over the day of the week\n",
    "non_business_production_sum_timeseries = merged_df.query('is_consumption == 0 & is_business == 0')[[\"datetime\", \"target\"]].groupby(\"datetime\").sum()\n",
    "non_business_production_sum_timeseries.reset_index(inplace=True)\n",
    "non_business_production_sum_timeseries = split_datetime(non_business_production_sum_timeseries, \"datetime\")\n",
    "# Have a look at the production average per day of the week\n",
    "non_business_production_av_day_of_week = non_business_production_sum_timeseries.groupby([\"day_of_week\", \"hour\"])[[\"target\"]].mean()\n",
    "non_business_production_av_day_of_week.reset_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(business_production_av_day_of_week.index, business_production_av_day_of_week[\"target\"], label=\"business\")\n",
    "plt.plot(non_business_production_av_day_of_week.index, non_business_production_av_day_of_week[\"target\"], label=\"non-business\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Production for businesses is higher on the weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sum of consumption of business producers and the average over the day of the week\n",
    "business_consumption_sum_timeseries = merged_df.query('is_consumption == 1 & is_business == 1')[[\"datetime\", \"target\"]].groupby(\"datetime\").sum()\n",
    "business_consumption_sum_timeseries.reset_index(inplace=True)\n",
    "business_consumption_sum_timeseries = split_datetime(business_consumption_sum_timeseries, \"datetime\")\n",
    "# Have a look at the consumption average per day of the week\n",
    "business_consumption_av_day_of_week = business_consumption_sum_timeseries.groupby([\"day_of_week\", \"hour\"])[[\"target\"]].mean()\n",
    "business_consumption_av_day_of_week.reset_index(inplace=True)\n",
    "\n",
    "# calculate sum of consumption of business producers and the average over the day of the week\n",
    "non_business_consumption_sum_timeseries = merged_df.query('is_consumption == 1 & is_business == 0')[[\"datetime\", \"target\"]].groupby(\"datetime\").sum()\n",
    "non_business_consumption_sum_timeseries.reset_index(inplace=True)\n",
    "non_business_consumption_sum_timeseries = split_datetime(non_business_consumption_sum_timeseries, \"datetime\")\n",
    "# Have a look at the consumption average per day of the week\n",
    "non_business_consumption_av_day_of_week = non_business_consumption_sum_timeseries.groupby([\"day_of_week\", \"hour\"])[[\"target\"]].mean()\n",
    "non_business_consumption_av_day_of_week.reset_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(business_consumption_av_day_of_week.index, business_consumption_av_day_of_week[\"target\"], label=\"business\")\n",
    "plt.plot(non_business_consumption_av_day_of_week.index, non_business_consumption_av_day_of_week[\"target\"], label=\"non-business\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_producers = merged_df.query('is_consumption == 0 & installed_capacity_client < 1000')\n",
    "small_producers_sum_timeseries = small_producers[[\"datetime\", \"county\", \"target\", \"surface_solar_radiation_downwards_forecast_weather\", \"installed_capacity_client\"]].groupby([\"county\", \"datetime\"]).agg({\"target\": \"sum\", \"surface_solar_radiation_downwards_forecast_weather\": \"mean\", \"installed_capacity_client\": \"mean\"})\n",
    "small_producers_sum_timeseries.reset_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=small_producers_sum_timeseries.query(\"county == 0\"), x=\"surface_solar_radiation_downwards_forecast_weather\", y=\"target\", hue=\"installed_capacity_client\")\n",
    "\n",
    "large_producers = merged_df.query('is_consumption == 0 & installed_capacity_client >= 1000')\n",
    "large_producers_sum_timeseries = large_producers[[\"datetime\", \"county\", \"target\", \"surface_solar_radiation_downwards_forecast_weather\", \"installed_capacity_client\"]].groupby([\"county\", \"datetime\"]).agg({\"target\": \"sum\", \"surface_solar_radiation_downwards_forecast_weather\": \"mean\", \"installed_capacity_client\": \"mean\"})\n",
    "large_producers_sum_timeseries.reset_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=large_producers_sum_timeseries.query(\"county == 0\"), x=\"surface_solar_radiation_downwards_forecast_weather\", y=\"target\", hue=\"installed_capacity_client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Solar radiation and production seems to be linear, but the line differs from how much capacity you have (more or less two lines running from the same origin, like a \"star\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(merged_df.groupby(['county', 'datetime', 'is_consumption']).mean(numeric_only=True).reset_index(),\n",
    "         x='datetime', y='target', color='county', facet_col='is_consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# trace 0 = surface solar radat\n",
    "# trace 1 = target\n",
    "# trace 2 = sum of installed capacity\n",
    "\n",
    "#df = merged_df.query('(is_business not in [1]) and (is_consumption == 0)').groupby(['datetime']).mean(numeric_only=True).reset_index()\n",
    "aggregations = {'target': 'sum', 'surface_solar_radiation_downwards_forecast_weather': 'mean', 'installed_capacity_client': 'sum'}\n",
    "try:\n",
    "    df = merged_df.query('(is_business not in [1]) and (is_consumption == 0)').groupby(['datetime']).aggregate(aggregations).reset_index()\n",
    "except:\n",
    "    print('mean applied')\n",
    "    df = merged_df.query('(is_business not in [1]) and (is_consumption == 0)').groupby(['datetime']).aggregate('mean').reset_index()\n",
    "\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['datetime'], y=df['surface_solar_radiation_downwards_forecast_weather'], opacity=0.7),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['datetime'], y=df['target'], opacity=0.7),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['datetime'], y=df['installed_capacity_client'], opacity=0.7),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypotheses and baseline model ideas\n",
    "\n",
    "- Simple Baseline Ideas\n",
    "  - **Simple baseline model**: Average (average) consumption of day before this year would prediction consumption for next year\n",
    "  - How is **weather of day before** is influencing production and consumption (if sunshine day before is low, production next day would possibly be lower)\n",
    "\n",
    "  - **Very simple regression model**, with few features (solar radiation colors, snow/temperature, capacity)\n",
    "- Correlation\n",
    "  - What are parameters for the **highest correlation for consumption** (e.g. higher solar radiation, the higher the production; the higher the capacity, the higher the production)\n",
    "  - Gas prices and electricity prices change -> correlation\n",
    "- **Two models**: production (solar and capacity), consumption (temperature); target represents if consumption or production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns for date / time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    # What columns are of type datetime?\n",
    "    datetime_columns = merged_df.select_dtypes(include='datetime64').columns\n",
    "    \n",
    "    for c in datetime_columns:\n",
    "        print(f\"Timezone for {c} is {data[c].dt.tz}\")\n",
    "\n",
    "    # Adding columns for date & time\n",
    "    data['year']    = data['datetime'].dt.year\n",
    "    data['quarter'] = data['datetime'].dt.quarter\n",
    "    data['month']   = data['datetime'].dt.month\n",
    "    data['week']    = data['datetime'].dt.isocalendar().week\n",
    "    data['hour']    = data['datetime'].dt.hour \n",
    "\n",
    "    data['day_of_year']  = data['datetime'].dt.day_of_year\n",
    "    data['day_of_month'] = data['datetime'].dt.day\n",
    "    data['day_of_week']  = data['datetime'].dt.day_of_week\n",
    "\n",
    "    return data\n",
    "\n",
    "merged_df = add_features(merged_df)\n",
    "\n",
    "## -> need to convert to EET timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "\n",
    "model ideas\n",
    "- calculate average of one year, use this as a prediction for next year ? (but we know already that seasonality is important)\n",
    "- ** prediction of t is equal to t-1year  **\n",
    "\n",
    "\n",
    "to do\n",
    "- define x and y, only 'model' on most recent year (2023?)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train.copy()\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Baseline Model\n",
    "\n",
    "test_offset = train.copy()\n",
    "test_offset\n",
    "test_offset['datetime'] = test_offset['datetime'] + pd.Timedelta(value=365, unit='days')  # PLUS or MINUS?\n",
    "\n",
    "test_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df with baseline predictions (in test_offset)\n",
    "test = test.merge(\n",
    "    test_offset, \n",
    "    on=['county', 'is_business', 'product_type', 'is_consumption', 'datetime'], \n",
    "    how='left', \n",
    "    suffixes=('', '_previous_year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: define function to pull target from 1 year ago\n",
    "\n",
    "'''def baseline(date):\n",
    "    prediction  = target from one year ago   \n",
    "\n",
    "    return prediction'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "print('Previous year as prediction:', mean_absolute_error(test['target'], test['target_previous_year']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define error fun\n",
    "def calc_mae(x, y):\n",
    "    mae = np.mean(np.abs(y - x))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate df for plotting\n",
    "plotdat = test.groupby(['datetime', 'is_consumption']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE consumption: ', calc_mae(plotdat.query('is_consumption == 1')['target'], plotdat.query('is_consumption == 1')['target_previous_year']))\n",
    "print('MAE production:  ', calc_mae(plotdat.query('is_consumption == 0')['target'], plotdat.query('is_consumption == 0')['target_previous_year']))\n",
    "\n",
    "print('MAE all:         ', calc_mae(plotdat['target'], plotdat['target_previous_year']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of pre-implemented sklearn MAE are different from the defined calc_mae function. It is unclear why, but Kaggle uses the pre-implemented function -> this is what we should optimize for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "test_prod = test.query('is_consumption == 0')\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_prod['datetime'], y=test_prod['target'], opacity=0.7, name='production_true'),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_prod['datetime'], y=test_prod['target_previous_year'], opacity=0.7, name='production_pred'),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual and baseline-predicted consumption\n",
    "\n",
    "plt.plot(plotdat.query('is_consumption == 1')['datetime'], plotdat.query('is_consumption == 1')['target'])\n",
    "plt.plot(plotdat.query('is_consumption == 1')['datetime'], plotdat.query('is_consumption == 1')['target_previous_year'], 'r')\n",
    "\n",
    "plt.legend(['obs.', 'pred.'])\n",
    "plt.title('Baseline model: observed and predicted consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual and baseline-predicted production\n",
    "\n",
    "plt.plot(plotdat.query('is_consumption == 0')['datetime'], plotdat.query('is_consumption == 0')['target'])\n",
    "plt.plot(plotdat.query('is_consumption == 0')['datetime'], plotdat.query('is_consumption == 0')['target_previous_year'], 'r')\n",
    "\n",
    "plt.legend(['obs.', 'pred.'])\n",
    "plt.title('Baseline model: observed and predicted production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual and baseline-predicted production\n",
    "# average over data block id, because during night (no sun), production is 0 and therefore plot becomes unreadable\n",
    "\n",
    "plotdat2 = plotdat.groupby(['data_block_id', 'is_consumption']).mean().reset_index()\n",
    "\n",
    "plt.plot(plotdat2.query('is_consumption == 0')['datetime'], plotdat2.query('is_consumption == 0')['target'])\n",
    "plt.plot(plotdat2.query('is_consumption == 0')['datetime'], plotdat2.query('is_consumption == 0')['target_previous_year'], 'r')\n",
    "\n",
    "plt.legend(['obs.', 'pred.'])\n",
    "plt.title('Baseline model: observed and predicted production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del client\n",
    "del gas_prices\n",
    "del electricity_prices\n",
    "del forecast_weather\n",
    "del historical_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df for modelling\n",
    "model_df = merged_df.copy()\n",
    "\n",
    "# model is not able to handle object type\n",
    "model_df.drop('time_of_day', axis=1, inplace=True)\n",
    "\n",
    "# split datetime into meaningful features of int types\n",
    "model_df = split_datetime(model_df)\n",
    "\n",
    "# model is not able to handle datetime\n",
    "model_df = model_df.drop(model_df.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, EET]']).columns, axis=1)\n",
    "\n",
    "# drop na from target\n",
    "model_df.dropna(subset=['target'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,  y_test = train_test_split(model_df.drop('target', axis=1), model_df['target'], test_size=0.3, random_state=0)\n",
    "\n",
    "bst = XGBRegressor(enable_categorical=True)\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "\n",
    "# main optimisation metric\n",
    "print('Mean absolute error test', mean_absolute_error(y_test, y_pred))\n",
    "print('Mean absolute error train', mean_absolute_error(y_train, bst.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first attempt gave us 50.75 mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['data_block_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split of old data to train and newer one to test\n",
    "\n",
    "Xy_train = model_df[model_df.data_block_id < 450]\n",
    "X_train = Xy_train.drop('target', axis=1)\n",
    "y_train = Xy_train.target\n",
    "\n",
    "Xy_test = model_df[model_df.data_block_id >= 450]\n",
    "X_test = Xy_test.drop('target', axis=1)\n",
    "y_test = Xy_test.target\n",
    "\n",
    "bst = XGBRegressor(enable_categorical=True)\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "\n",
    "# main optimisation metric\n",
    "print('Mean absolute error test', mean_absolute_error(y_test, y_pred))\n",
    "print('Mean absolute error train', mean_absolute_error(y_train, bst.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by dates and use newer ones for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {key: value for key, value in zip(bst.feature_names_in_, bst.feature_importances_)}\n",
    "features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hours_ahead_forecast treated as important feature, probably smth to drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualisation\n",
    "- split by date\n",
    "- tweaking the parameters\n",
    "- drop some features\n",
    "- feature engineering\n",
    "- overfitting with traditional train_test_split?\n",
    "- try to models/ multiple_output/ other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df2 = model_df.copy()\n",
    "# model_df2.drop(['row_id', ])\n",
    "\n",
    "split_datablock = 300\n",
    "\n",
    "Xy_train = model_df[model_df.data_block_id < split_datablock]\n",
    "X_train = Xy_train.drop('target', axis=1)\n",
    "y_train = Xy_train.target\n",
    "\n",
    "Xy_test = model_df[model_df.data_block_id >= split_datablock]\n",
    "X_test = Xy_test.drop('target', axis=1)\n",
    "y_test = Xy_test.target\n",
    "\n",
    "bst = XGBRegressor(enable_categorical=True)\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = bst.predict(X_test)\n",
    "y_pred_train = bst.predict(X_train)\n",
    "\n",
    "# main optimisation metric\n",
    "print('Mean absolute error test', mean_absolute_error(y_test, y_pred_test))\n",
    "print('Mean absolute error train', mean_absolute_error(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=Xy_train.index, y=y_pred_train-y_train, color=Xy_train.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=Xy_test, x=Xy_test.index, y=y_pred_test-y_test, color=Xy_test.month, hover_data='day_of_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test['residual'] = y_pred_test-y_test\n",
    "\n",
    "Xy_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Xy_test.corr(numeric_only=True), annot=False, cmap='RdBu', center = 0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.figure(\n",
    "    figsize=(20, 20)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'residual'\n",
    "\n",
    "# Exclude non-numeric columns\n",
    "numeric_columns = Xy_test.select_dtypes(include=['number']).columns\n",
    "numeric_df = Xy_test[numeric_columns]\n",
    "numeric_df_cons = numeric_df[numeric_df['is_consumption'] == 1]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_df_cons.corr()\n",
    "\n",
    "# Select correlations based on the threshold\n",
    "threshold = 0.15\n",
    "significant_correlations = correlation_matrix[(correlation_matrix[target_column] > threshold) | (correlation_matrix[target_column] < -threshold)][target_column]\n",
    "\n",
    "# Plot a heatmap of the significant correlations with the target\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(significant_correlations.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", cbar=False)\n",
    "plt.title(f'Significant Correlations with {target_column}, CONSUM ONLY (Threshold: {threshold})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'residual'\n",
    "\n",
    "# Exclude non-numeric columns\n",
    "numeric_columns = Xy_test.select_dtypes(include=['number']).columns\n",
    "numeric_df = Xy_test[numeric_columns]\n",
    "numeric_df_cons = numeric_df[numeric_df['is_consumption'] == 0]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_df_cons.corr()\n",
    "\n",
    "# Select correlations based on the threshold\n",
    "threshold = 0.15\n",
    "significant_correlations = correlation_matrix[(correlation_matrix[target_column] > threshold) | (correlation_matrix[target_column] < -threshold)][target_column]\n",
    "\n",
    "# Plot a heatmap of the significant correlations with the target\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(significant_correlations.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", cbar=False)\n",
    "plt.title(f'Significant Correlations with {target_column}, PRODUCTION ONLY (Threshold: {threshold})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- residuals are bigger at the summer time, we quess because production is happening at this time\n",
    "\n",
    "- residuals on the test data have weekly pattern\n",
    "- last two month predicted very poorly\n",
    "\n",
    "- residuals are different depending on how we split our data \n",
    "- we see unexpalinable patterns in residuals\n",
    "- residuals for consumption and production correlate with different features\n",
    "\n",
    "- try residual analysis with traditional test_train_split\n",
    "\n",
    "- tweak the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,  y_test = train_test_split(model_df.drop('target', axis=1), model_df['target'], test_size=0.3, random_state=0)\n",
    "\n",
    "bst = XGBRegressor(enable_categorical=True)\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "\n",
    "# main optimisation metric\n",
    "print('Mean absolute error test', mean_absolute_error(y_test, y_pred))\n",
    "print('Mean absolute error train', mean_absolute_error(y_train, bst.predict(X_train)))\n",
    "\n",
    "y_pred_test = bst.predict(X_test)\n",
    "y_pred_train = bst.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=X_train.index, y=y_pred_train-y_train, color=X_train.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=X_test, x=X_test.index, y=y_pred_test-y_test, color=X_test.month, hover_data='day_of_week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pretty much the same patterns with different train test splits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
