{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Add the '../imports' directory to the sys.path list\n",
    "import sys\n",
    "sys.path.append('../imports')\n",
    "from helper_functions import split_datetime\n",
    "from actpred_plot import plot_actual_vs_pred\n",
    "from data_preprocessing import merge_data, remove_col\n",
    "from feature_engineering import * # this is bad practice, call functions explicitly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# Read CSVs and parse relevant date columns\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "client_train = pd.read_csv(DATA_DIR + \"client.csv\")\n",
    "historical_weather_train = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
    "forecast_weather_train = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
    "electricity_prices_train = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
    "gas_prices_train = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
    "weather_station_to_county_mapping = pd.read_csv(DATA_DIR + 'weather_station_to_county_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge all DataFrames \n",
    "merged_train_df = merge_data(train, client_train, historical_weather_train,\n",
    "        forecast_weather_train, electricity_prices_train, gas_prices_train, weather_station_to_county_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all non needed columns (ids and timestamps)\n",
    "model_df = remove_col(merged_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop('target', axis=1)\n",
    "y = model_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the parameters from the model optimization (notebook 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(enable_categorical=True, max_depth=9, learning_rate=0.3) \n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusion on test/train split\n",
    "- with the parameters set by grid search, the sizes of test and train data do not influence the quality of model results too much. \n",
    "- random state is not too important \n",
    "- using XGBoot's default parameters, the size makes a small difference. (Note: we did the grid search with all features, probably including ID columns...)\n",
    "\n",
    "- Already here, we are overfitting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# main optimisation metric\n",
    "print('Mean absolute error test', mean_absolute_error(y_test, y_test_pred))\n",
    "print('Mean absolute error train', mean_absolute_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- residuals are bigger at the summer time, we quess because production is happening at this time\n",
    "\n",
    "- residuals on the test data have weekly pattern\n",
    "- last two month predicted very poorly\n",
    "\n",
    "- residuals are different depending on how we split our data \n",
    "- we see unexpalinable patterns in residuals\n",
    "- residuals for consumption and production correlate with different features\n",
    "\n",
    "- try residual analysis with traditional test_train_split\n",
    "\n",
    "- tweak the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We tried to split the data into old and new data for training and testing. Because of bad scores we continue with a random train-test-split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking: What features are important (or not)\n",
    "\n",
    "- prices (should not influence consumption or production)\n",
    "\n",
    "- we should check correlation heat maps to identify parameters that are highly correlated (although XGBoost seems to be able to handle correlated features well)\n",
    "- this way we might be able to eliminate some weather related features that cover the same 'topic'\n",
    "\n",
    "\n",
    "'county',                                   \n",
    " 'is_business',\n",
    " 'product_type',\n",
    " 'target',\n",
    " 'is_consumption',\n",
    " 'eic_count_client',\n",
    " 'installed_capacity_client',\n",
    " 'lowest_price_per_mwh_gas_prices',             -> maybe drop the prices, we think prices correlate with season more than consumer behavior \n",
    " 'highest_price_per_mwh_gas_prices',\n",
    " 'euros_per_mwh_electricity_prices',\n",
    " 'temperature_hist_weather',\n",
    " 'dewpoint_hist_weather',\n",
    " 'rain_hist_weather',                           -> rain may be not too important (can only happen with cloud cover anyway, and cloud cover may \n",
    " 'snowfall_hist_weather',                               be a better feature)\n",
    " 'surface_pressure_hist_weather',\n",
    " 'cloudcover_total_hist_weather',            -> cloud cover influences solar radiation... is maybe redundant\n",
    " 'cloudcover_low_hist_weather',                   drop other cloud covers and only keep total\n",
    " 'cloudcover_mid_hist_weather',\n",
    " 'cloudcover_high_hist_weather',\n",
    " 'windspeed_10m_hist_weather',\n",
    " 'winddirection_10m_hist_weather',\n",
    " 'shortwave_radiation_hist_weather',\n",
    " 'direct_solar_radiation_hist_weather',\n",
    " 'diffuse_radiation_hist_weather',\n",
    " 'temperature_forecast_weather',\n",
    " 'dewpoint_forecast_weather',\n",
    " 'cloudcover_high_forecast_weather',\n",
    " 'cloudcover_low_forecast_weather',\n",
    " 'cloudcover_mid_forecast_weather',\n",
    " 'cloudcover_total_forecast_weather',\n",
    " '10_metre_u_wind_component_forecast_weather',\n",
    " '10_metre_v_wind_component_forecast_weather',\n",
    " 'direct_solar_radiation_forecast_weather',\n",
    " 'surface_solar_radiation_downwards_forecast_weather',\n",
    " 'snowfall_forecast_weather',\n",
    " 'total_precipitation_forecast_weather',\n",
    " 'year',\n",
    " 'month',\n",
    " 'week',\n",
    " 'hour',\n",
    " 'day_of_year',\n",
    " 'day_of_month',\n",
    " 'day_of_week']                             -> seemed important in EDA and in feature importance plot    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected features, as found by the sequential feature selector in 02.2_Model_Optimization.ipynb:\n",
    "\n",
    "['county', 'is_business', 'product_type', 'is_consumption',\n",
    "       'eic_count_client', 'installed_capacity_client',\n",
    "       'rain_hist_weather', 'snowfall_hist_weather',\n",
    "       'cloudcover_total_hist_weather', 'cloudcover_mid_hist_weather',\n",
    "       'cloudcover_high_hist_weather', 'diffuse_radiation_hist_weather',\n",
    "       'temperature_forecast_weather', 'dewpoint_forecast_weather',\n",
    "       'surface_solar_radiation_downwards_forecast_weather',\n",
    "       'total_precipitation_forecast_weather', 'year', 'week', 'hour',\n",
    "       'day_of_year', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-coded list of selected features, found by sequential feature selector\n",
    "\n",
    "sel_features = ['county', 'is_business', 'product_type', 'is_consumption',\n",
    "       'eic_count_client', 'installed_capacity_client',\n",
    "       'rain_hist_weather', 'snowfall_hist_weather',\n",
    "       'cloudcover_total_hist_weather', 'cloudcover_mid_hist_weather',\n",
    "       'cloudcover_high_hist_weather', 'diffuse_radiation_hist_weather',\n",
    "       'temperature_forecast_weather', 'dewpoint_forecast_weather',\n",
    "       'surface_solar_radiation_downwards_forecast_weather',\n",
    "       'total_precipitation_forecast_weather', 'year', 'week', 'hour',\n",
    "       'day_of_year', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae_tst_trn(y_test, y_test_pred, y_train, y_train_pred):\n",
    "    '''Calculates MAE for test and train data.'''\n",
    "    \n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    print('Mean absolute error test', mae_test)\n",
    "    print('Mean absolute error train', mae_train)\n",
    "    print('MAE Difference (test-train)', mae_test-mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df):\n",
    "    # Run model with selected features\n",
    "    ## do stuff to model_df\n",
    "\n",
    "    # train-test split\n",
    "    X = df\n",
    "    y = model_df['target']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # training\n",
    "    model = XGBRegressor(enable_categorical=True) \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluation\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    calc_mae_tst_trn(y_test, y_test_pred, y_train, y_train_pred)\n",
    "\n",
    "df = model_df.copy()\n",
    "# only keep columns selected by SFS\n",
    "cols = sel_features\n",
    "df = df[cols]\n",
    "\n",
    "run_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> these are the best results so far (not in terms of absolute values for train, but in terms of preventing overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae_prodcons(target_production, target_prediction_production, target_consumption, target_prediction_consumption):\n",
    "    ''' Function that calculates MAE for production and consumption seperately\n",
    "    '''\n",
    "    print('Mean absolute error test production', mean_absolute_error(target_production, target_prediction_production))\n",
    "    print('Mean absolute error test consumption', mean_absolute_error(target_consumption, target_prediction_consumption))\n",
    "\n",
    "    print('Mean absolute percentage error test production', mean_absolute_percentage_error(target_production, target_prediction_production))\n",
    "    print('Mean absolute percentage error test consumption', mean_absolute_percentage_error(target_consumption,target_prediction_consumption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MAE for prod and cons separately\n",
    "\n",
    "X_test_prod = X_test[X_test[\"is_consumption\"] == 0]\n",
    "y_test_prod = y_test[X_test[\"is_consumption\"] == 0]\n",
    "y_test_pred_prod = y_test_pred[X_test[\"is_consumption\"] == 0]\n",
    "\n",
    "X_test_con = X_test[X_test[\"is_consumption\"] == 1]\n",
    "y_test_con = y_test[X_test[\"is_consumption\"] == 1]\n",
    "y_test_pred_con = y_test_pred[X_test[\"is_consumption\"] == 1]\n",
    "\n",
    "calc_mae_prodcons(y_test_prod, y_test_pred_prod, y_test_con, y_test_pred_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> predicting consumption is more accurate than predicting production (we have seen in EDA that there is a lot of fluctuation in production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the index of X_test to merge the target values back from the original dataset\n",
    "X_test_with_target = X_test\n",
    "X_test_with_target['target'] = y_test\n",
    "X_test_with_target['predicted'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_with_target.query('is_consumption==0 & target == 0').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "sel_features = ['county', 'is_business', 'product_type', 'is_consumption',\n",
    "       'eic_count_client', 'installed_capacity_client',\n",
    "       'rain_hist_weather', 'snowfall_hist_weather',\n",
    "       'cloudcover_total_hist_weather', 'cloudcover_mid_hist_weather',\n",
    "       'cloudcover_high_hist_weather', 'diffuse_radiation_hist_weather',\n",
    "       'temperature_forecast_weather', 'dewpoint_forecast_weather',\n",
    "       'surface_solar_radiation_downwards_forecast_weather',\n",
    "       'total_precipitation_forecast_weather', 'year', 'week', 'hour',\n",
    "       'day_of_year', 'day_of_week']\n",
    "\n",
    "IDEAS \n",
    "- holidays (christmas, easter, ...)\n",
    "- workday vs weekend\n",
    "- increasing in the capacity (??)\n",
    "- weather aggregation for previous/future period\n",
    "- prices aggregation for previous period (discard)\n",
    "\n",
    "- daylight (or not), possibly boolean? ---> if false, set production 0\n",
    "       - based on time, sunlight, cloudcover\n",
    "       - essentially solar radiation should be enough, it already indicates if the sun is shining or not. \n",
    "- capacity per client ('eic_count' / )\n",
    "- square the capacity, to give it a bigger weight (to exaggerate the trend in increasing capacity)\n",
    "- some scale that represents the growth in capacity (could be different in the segments?)\n",
    "- how much of the capacity is actually used at any time (capacity / production)\n",
    "\n",
    "TO DO\n",
    "- replace negative targets with 0 (neither production nor consumption can be negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best so far (Vanessa/Sarah commit):\n",
    "- Mean absolute error test 48.36763815779347\n",
    "- Mean absolute error train 47.16665900818055\n",
    "- MAE Difference (test-train) 1.2009791496129196\n",
    "\n",
    "STEP 1: With dropping cloudcover and diffuse:\n",
    "- Mean absolute error test 48.00836528113775\n",
    "- Mean absolute error train 46.826012754281024\n",
    "- MAE Difference (test-train) 1.1823525268567252\n",
    "\n",
    "STEP2+STEP 3: With new col temp_dew:\n",
    "- Mean absolute error test 47.46072403111012\n",
    "- Mean absolute error train 46.24984824354659\n",
    "- MAE Difference (test-train) 1.2108757875635305"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = add_daylight_col(model_df)\n",
    "\n",
    "model_df = add_capacity_col(model_df)\n",
    "\n",
    "model_df = basic_improvements(model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to change the list of selected features\n",
    "\n",
    "selected_features = ['county', 'is_business', 'product_type', 'is_consumption',\n",
    "       'eic_count_client',\n",
    "       'surface_solar_radiation_downwards_forecast_weather',\n",
    "       'total_precipitation_forecast_weather', 'year', 'week', 'hour',\n",
    "       'day_of_year', 'day_of_week','daylight', 'capacity_per_eic',\n",
    "       'squared_capacity_client', 'sum_column', 'temp_dew'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X = model_df.drop(\"target\", axis=1)[selected_features]\n",
    "y = model_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# training\n",
    "model = XGBRegressor(enable_categorical=True) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "calc_mae_tst_trn(y_test, y_test_pred, y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted_target(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # reintroduce the datetime column for merging\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day_of_month', 'hour']].rename(columns={'day_of_month' : 'day'}),format='%Y:%m:%d:%h')\n",
    "    # make copy of the needed columns\n",
    "    shifted_df = df[['county', 'is_business', 'product_type', 'is_consumption','datetime', 'target']].copy()\n",
    "    # rename the target as shifted_target - that is the new column we want to add\n",
    "    shifted_df.rename(columns={'target' : 'shifted_target'}, inplace=True)\n",
    "    # shift the datetime by two days for our helper df\n",
    "    shifted_df[\"datetime\"] = shifted_df[\"datetime\"] + pd.Timedelta(2, unit=\"days\")\n",
    "    # merge the shifted df to our original df - match the target of today to the day two days ahead \n",
    "    df = pd.merge(df, shifted_df, on= ['county', 'is_business', 'product_type', 'is_consumption','datetime'], how='left')\n",
    "    # drop the datetime column again\n",
    "    df.drop(\"datetime\", axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = prepro_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = add_shifted_target(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['datetime'] = pd.to_datetime(test[['year', 'month', 'day_of_month', 'hour']].rename(columns={'day_of_month' : 'day'}),format='%Y:%m:%d:%h')\n",
    "# test.groupby(['county', 'is_business', 'product_type', 'is_consumption','datetime'])[[\"target\", \"shifted_target\"]].sum().tail(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023-05-29 20:00:00\t267.423\t141.646  \n",
    "2023-05-29 21:00:00\t296.073\t172.973  \n",
    "2023-05-29 22:00:00\t299.806\t190.316  \n",
    "2023-05-29 23:00:00\t177.056\t183.756  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to change again the list of selected features\n",
    "\n",
    "selected_features = ['county', 'is_business', 'product_type', 'is_consumption',\n",
    "       'eic_count_client',\n",
    "       'surface_solar_radiation_downwards_forecast_weather',\n",
    "       'total_precipitation_forecast_weather', 'year', 'week', 'hour',\n",
    "       'day_of_year', 'day_of_week','daylight', 'capacity_per_eic',\n",
    "       'squared_capacity_client', 'sum_column', 'temp_dew', 'shifted_target'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X = test.drop(\"target\", axis=1)[selected_features]\n",
    "y = test['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# training\n",
    "model = XGBRegressor(enable_categorical=True) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "calc_mae_tst_trn(y_test, y_test_pred, y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last scores:  \n",
    "Mean absolute error test 47.515057300563946  \n",
    "Mean absolute error train 46.501235997796435  \n",
    "MAE Difference (test-train) 1.0138213027675107  \n",
    "\n",
    "> Small improvement but bigger gab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = model_df[sel_features].select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(y_test, y_test_pred):\n",
    "    \"\"\"Generated true vs. predicted values and residual scatter plot for models\n",
    "\n",
    "    Args:\n",
    "        y_test (array): true values for y_test\n",
    "        y_pred_test (array): predicted values of model for y_test\n",
    "    \"\"\"     \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_test_pred\n",
    "    \n",
    "    # Plot real vs. predicted values \n",
    "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "    plt.subplots_adjust(right=1)\n",
    "    plt.suptitle('Error Analysis')\n",
    "    \n",
    "    ax[0].scatter(y_test_pred, y_test, color=\"#FF5A36\", alpha=0.7)\n",
    "    ax[0].plot([-400, y_test_pred.max()+10], [-400, y_test_pred.max()+10], color=\"#193251\")\n",
    "    ax[0].set_title(\"True vs. predicted values\", fontsize=16)\n",
    "    ax[0].set_xlabel(\"predicted values\")\n",
    "    ax[0].set_ylabel(\"true values\")\n",
    "    # ax[0].set_xlim((y_pred_test.min()-10), (y_pred_test.max()+10))\n",
    "    # ax[0].set_ylim((y_test.min()-40), (y_test.max()+40))\n",
    "    ax[0].set_xlim((y_test_pred.min()-10), (y_test_pred.max()+10))\n",
    "    ax[0].set_ylim((y_test_pred.min()-10), (y_test_pred.max()+10))\n",
    "\n",
    "\n",
    "    ax[1].scatter(y_test_pred, residuals, color=\"#FF5A36\", alpha=0.7)\n",
    "    ax[1].plot([-400, y_test_pred.max()+10], [0,0], color=\"#193251\")\n",
    "    ax[1].set_title(\"Residual Scatter Plot\", fontsize=16)\n",
    "    ax[1].set_xlabel(\"predicted values\")\n",
    "    ax[1].set_ylabel(\"residuals\")\n",
    "    ax[1].set_xlim((y_test_pred.min()-10), (y_test_pred.max()+10))\n",
    "    ax[1].set_ylim((residuals.min()-10), (residuals.max()+10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
