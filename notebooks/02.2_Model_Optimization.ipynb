{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Add the '../imports' directory to the sys.path list\n",
    "import sys\n",
    "sys.path.append('../imports')\n",
    "from helper_functions import split_datetime\n",
    "from data_preprocessing import merge_data, remove_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# Read CSVs and parse relevant date columns\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "client_train = pd.read_csv(DATA_DIR + \"client.csv\")\n",
    "historical_weather_train = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
    "forecast_weather_train = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
    "electricity_prices_train = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
    "gas_prices_train = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
    "weather_station_to_county_mapping = pd.read_csv(DATA_DIR + 'weather_station_to_county_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge all DataFrames \n",
    "merged_train_df = merge_data(train, client_train, historical_weather_train,\n",
    "        forecast_weather_train, electricity_prices_train, gas_prices_train, weather_station_to_county_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all non needed columns (ids and timestamps)\n",
    "model_df = remove_col(merged_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Parameters\n",
    "\n",
    "What parameters can we tune?\n",
    "Source: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n",
    "- `booster` [default: `gbtree`]: \n",
    "  - Description: Specifies the booster type to use.\n",
    "  - Options: \n",
    "    - `gbtree`: Uses tree-based models.\n",
    "    - `dart`: Similar to `gbtree`, but with dropout.\n",
    "    - `gblinear`: Uses linear functions.\n",
    "    \n",
    "- `eta` [default: `0.3`, alias: `learning_rate`]: \n",
    "  - Description: Step size shrinkage used in update to prevent overfitting. \n",
    "  - Range: `[0, 1]`\n",
    "\n",
    "- `max_depth` [default: `6`]: \n",
    "  - Description: Maximum depth of a tree. Increasing this value will make the model more complex and likely to overfit. \n",
    "  - Range: `[0, ∞]` (0 indicates no limit)\n",
    "\n",
    "- `subsample` [default: `1`]: \n",
    "  - Description: Subsample ratio of the training instances to prevent overfitting. \n",
    "  - Range: `(0, 1]`\n",
    "\n",
    "- `lambda` [default: `1`, alias: `reg_lambda`]: \n",
    "  - Description: L2 regularization term on weights. \n",
    "  - Range: `[0, ∞]`\n",
    "\n",
    "- `alpha` [default: `0`, alias: `reg_alpha`]: \n",
    "  - Description: L1 regularization term on weights. \n",
    "  - Range: `[0, ∞]`\n",
    "\n",
    "- `eval_metric` [default: according to objective]: \n",
    "  - Description: Evaluation metrics for validation data. \n",
    "  - Note: A default metric is assigned according to the objective (e.g., `rmse` for regression, `logloss` for classification). Users can add multiple evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "First only searching different tree level depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning with gridsearch\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train,  y_test = train_test_split(model_df.drop('target', axis=1), model_df['target'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Define a range of hyperparameters to tune\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    #'n_estimators': [100, 200, 300, 500],\n",
    "    #'subsample': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Initialize the XGBRegressor with enable_categorical=True\n",
    "xgb_reg = XGBRegressor(enable_categorical=True)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of grid search (best model) as pickle, so we can call it for the test data in modelling_test_data.ipynb\n",
    "with open('../models/XGBoost_first_best_model.pickle', 'wb') as file:\n",
    "    pickle.dump(best_model,  file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We should run the grid search again, but with the reduced number of columns\n",
    "(don't forget)\n",
    "- reduce overfitting? (how?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking best model's MAE on test set\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Absolute Error between the actual and predicted values\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking best model's MAE on train set\n",
    "\n",
    "y_pred = best_model.predict(X_train)\n",
    "\n",
    "# Calculate the Mean Absolute Error between the actual and predicted values\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearch\n",
    "Different parameters are tuned, and df is split into consumption/production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search, but splitting the df into consumption/production, and choosing different parameters for tuning\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "drop_columns = [\n",
    "    'target',\n",
    "    'hours_ahead_forecast_weather',\n",
    "    'row_id',\n",
    "    'data_block_id',\n",
    "    'prediction_unit_id',\n",
    "    'longitude_hist_weather',\n",
    "    'longitude_forecast_weather',\n",
    "    'latitude_hist_weather',\n",
    "    'latitude_forecast_weather'\n",
    "]\n",
    "# max_depth 15 leads to overfitting\n",
    "params = {\n",
    "    'gamma': [0, 0.1, 1, 10],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_child_weight': [0, 1, 4, 8],\n",
    "    'lambda': [0, 0.01, 0.1, 1],\n",
    "    'num_parallel_tree': [1, 2, 3],\n",
    "}\n",
    "# consumption model\n",
    "X_train, X_test, y_train_cons,  y_test_cons = train_test_split(\n",
    "    model_df.drop(drop_columns, axis=1).query('is_consumption == 1'),\n",
    "    model_df.query('is_consumption == 1')['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bst_cons = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(enable_categorical=True),\n",
    "    param_distributions=params,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_iter=10,\n",
    "    cv=2\n",
    ")\n",
    "bst_cons.fit(X_train, y_train_cons)\n",
    "y_pred_test_cons = bst_cons.predict(X_test)\n",
    "y_pred_train_cons = bst_cons.predict(X_train)\n",
    "print('Mean absolute error train consumption', mean_absolute_error(y_train_cons, y_pred_train_cons))\n",
    "print('Mean absolute error test consumption', mean_absolute_error(y_test_cons, y_pred_test_cons))\n",
    "# production model\n",
    "X_train, X_test, y_train_prod,  y_test_prod = train_test_split(\n",
    "    model_df.drop(drop_columns, axis=1).query('is_consumption == 0'),\n",
    "    model_df.query('is_consumption == 0')['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bst_prod = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(enable_categorical=True),\n",
    "    param_distributions=params,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_iter=10,\n",
    "    cv=2\n",
    ")\n",
    "bst_prod.fit(X_train, y_train_prod)\n",
    "y_pred_test_prod = bst_prod.predict(X_test)\n",
    "y_pred_train_prod = bst_prod.predict(X_train)\n",
    "print('Mean absolute error train production', mean_absolute_error(y_train_prod, y_pred_train_prod))\n",
    "print('Mean absolute error test production', mean_absolute_error(y_test_prod, y_pred_test_prod))\n",
    "# overall score\n",
    "print(\n",
    "    'Mean absolute error train overall',\n",
    "    mean_absolute_error(\n",
    "          pd.concat([pd.Series(y_train_cons), pd.Series(y_train_prod)]),\n",
    "          pd.concat([pd.Series(y_pred_train_cons), pd.Series(y_pred_train_prod)])\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    'Mean absolute error test overall',\n",
    "    mean_absolute_error(\n",
    "        pd.concat([pd.Series(y_test_cons), pd.Series(y_test_prod)]),\n",
    "        pd.concat([pd.Series(y_pred_test_cons), pd.Series(y_pred_test_prod)])\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE is quite similar with the two hyperparameter search, max tree depth level is probably somewhere between 8 and 10.\n",
    "We need to validate our model on the test dataset, to see its reliability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
